{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the table `compaints_users` for the complaint text and product ID, and the `products` table to find the product and sub-product\n",
    "df = pd.read_csv('data/complaints_users.csv')\n",
    "df2 = pd.read_csv('data/products.csv')\n",
    "df = df.merge(df2, how='left', on='PRODUCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sub-product of \"I do not know\" to null\n",
    "df.loc[df['SUB_PRODUCT'] == 'I do not know', 'SUB_PRODUCT'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs for which there have been no recent complaints\n",
    "df['date_norm'] = df['DATE'].apply(lambda x: int(x[6:10] + x[0:2] + x[3:5]))\n",
    "df = df[df['date_norm'] > 20180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complaints which have null values in either main or sub products\n",
    "df = df[(df['MAIN_PRODUCT'].notnull()) & (df['SUB_PRODUCT'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs which have fewer than 1000 complaints\n",
    "df['COMPLAINT_COUNTS'] = df.groupby('SUB_PRODUCT')['COMPLAINT_ID'].transform('count')\n",
    "df = df[df['COMPLAINT_COUNTS'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118302 total rows\n",
      "9 unique main-products\n",
      "17 unique sub-products\n"
     ]
    }
   ],
   "source": [
    "print(len(df), 'total rows')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 1000]['MAIN_PRODUCT'].unique()), 'unique main-products')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 1000]['SUB_PRODUCT'].unique()), 'unique sub-products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "    54735 Credit reporting\n",
      "    1030 Other personal consumer report\n",
      "    1 Conventional home mortgage\n",
      "\n",
      "Debt collection\n",
      "    4964 Medical debt\n",
      "    8401 Other debt\n",
      "    6799 Credit card debt\n",
      "    1123 Payday loan debt\n",
      "    1095 Auto debt\n",
      "\n",
      "Student loan\n",
      "    3956 Federal student loan servicing\n",
      "    2197 Private student loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "    2403 Store credit card\n",
      "    10359 General-purpose credit card or charge card\n",
      "\n",
      "Mortgage\n",
      "    6947 Conventional home mortgage\n",
      "    2288 FHA mortgage\n",
      "\n",
      "Checking or savings account\n",
      "    6745 Checking account\n",
      "\n",
      "Money transfer, virtual currency, or money service\n",
      "    1129 Domestic (US) money transfer\n",
      "\n",
      "Vehicle loan or lease\n",
      "    3024 Loan\n",
      "\n",
      "Payday loan, title loan, or personal loan\n",
      "    1106 Installment loan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "#     print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "#     print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print('   ', len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove that possibly mis-classified \"Convential home mortgage\" in \"Credit reporting...\" main-product\n",
    "df = (df[((df['MAIN_PRODUCT'] != 'Credit reporting, credit repair services, or other personal consumer reports') |\n",
    "     (df['SUB_PRODUCT'] != 'Conventional home mortgage'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118301"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "    54735 Credit reporting\n",
      "    1030 Other personal consumer report\n",
      "\n",
      "Debt collection\n",
      "    4964 Medical debt\n",
      "    8401 Other debt\n",
      "    6799 Credit card debt\n",
      "    1123 Payday loan debt\n",
      "    1095 Auto debt\n",
      "\n",
      "Student loan\n",
      "    3956 Federal student loan servicing\n",
      "    2197 Private student loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "    2403 Store credit card\n",
      "    10359 General-purpose credit card or charge card\n",
      "\n",
      "Mortgage\n",
      "    6947 Conventional home mortgage\n",
      "    2288 FHA mortgage\n",
      "\n",
      "Checking or savings account\n",
      "    6745 Checking account\n",
      "\n",
      "Money transfer, virtual currency, or money service\n",
      "    1129 Domestic (US) money transfer\n",
      "\n",
      "Vehicle loan or lease\n",
      "    3024 Loan\n",
      "\n",
      "Payday loan, title loan, or personal loan\n",
      "    1106 Installment loan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "#     print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "#     print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print('   ', len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT']], test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106470"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11831"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49582</th>\n",
       "      <td>I have tried on several occasions to dispute a...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40517</th>\n",
       "      <td>This complaint is in regards to overdraft fees...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25060</th>\n",
       "      <td>My old car was repossessed after I became unem...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14067</th>\n",
       "      <td>someone else used my information to obtained c...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18491</th>\n",
       "      <td>Back in XX/XX/XXXX per Ace Cash Express I took...</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          COMPLAINT_TEXT  \\\n",
       "49582  I have tried on several occasions to dispute a...   \n",
       "40517  This complaint is in regards to overdraft fees...   \n",
       "25060  My old car was repossessed after I became unem...   \n",
       "14067  someone else used my information to obtained c...   \n",
       "18491  Back in XX/XX/XXXX per Ace Cash Express I took...   \n",
       "\n",
       "                                            MAIN_PRODUCT       SUB_PRODUCT  \n",
       "49582                                    Debt collection  Credit card debt  \n",
       "40517                        Checking or savings account  Checking account  \n",
       "25060  Credit reporting, credit repair services, or o...  Credit reporting  \n",
       "14067  Credit reporting, credit repair services, or o...  Credit reporting  \n",
       "18491          Payday loan, title loan, or personal loan  Installment loan  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['COMPLAINT_TEXT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "train_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [doc.lower().replace(r'\\n', '',) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count=bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "def get_corpus(texts):\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 30\n",
    "\n",
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus,\n",
    "                           num_topics=number_of_topics,\n",
    "                           id2word=train_id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.115*\"number\" + 0.109*\"address\" + 0.099*\"name\" + 0.049*\"information\" + 0.028*\"phone\" + 0.024*\"social_security\" + 0.023*\"use\" + 0.020*\"never\" + 0.019*\"live\" + 0.017*\"personal\" + 0.012*\"contact\" + 0.012*\"list\" + 0.012*\"person\" + 0.010*\"paypal\" + 0.010*\"also\"'),\n",
       " (1,\n",
       "  '0.198*\"card\" + 0.058*\"chase\" + 0.039*\"close\" + 0.026*\"open\" + 0.023*\"offer\" + 0.022*\"use\" + 0.017*\"apply\" + 0.017*\"limit\" + 0.016*\"new\" + 0.015*\"receive\" + 0.014*\"american_express\" + 0.012*\"purchase\" + 0.012*\"point\" + 0.011*\"balance\" + 0.010*\"month\"'),\n",
       " (2,\n",
       "  '0.064*\"fee\" + 0.053*\"charge\" + 0.050*\"balance\" + 0.044*\"pay\" + 0.036*\"amount\" + 0.035*\"interest\" + 0.030*\"payment\" + 0.024*\"statement\" + 0.023*\"month\" + 0.014*\"due\" + 0.011*\"monthly\" + 0.011*\"make\" + 0.011*\"apply\" + 0.010*\"total\" + 0.010*\"would\"'),\n",
       " (3,\n",
       "  '0.103*\"insurance\" + 0.062*\"company\" + 0.060*\"claim\" + 0.027*\"policy\" + 0.026*\"services\" + 0.018*\"provide\" + 0.018*\"florida\" + 0.018*\"send\" + 0.017*\"request\" + 0.017*\"pay\" + 0.016*\"office\" + 0.014*\"cover\" + 0.013*\"agent\" + 0.010*\"copy\" + 0.010*\"damage\"'),\n",
       " (4,\n",
       "  '0.142*\"bank\" + 0.081*\"check\" + 0.047*\"money\" + 0.037*\"fund\" + 0.031*\"deposit\" + 0.029*\"day\" + 0.026*\"checking\" + 0.017*\"citibank\" + 0.016*\"cash\" + 0.016*\"transaction\" + 0.015*\"close\" + 0.015*\"transfer\" + 0.015*\"branch\" + 0.012*\"bonus\" + 0.012*\"hold\"'),\n",
       " (5,\n",
       "  '0.098*\"equifax\" + 0.056*\"fraudulent\" + 0.052*\"remove\" + 0.044*\"file\" + 0.039*\"fraud\" + 0.030*\"open\" + 0.029*\"police\" + 0.029*\"theft\" + 0.024*\"belong\" + 0.021*\"identity_theft\" + 0.019*\"victim_identity\" + 0.016*\"item\" + 0.016*\"ftc\" + 0.015*\"dispute\" + 0.014*\"mine\"'),\n",
       " (6,\n",
       "  '0.169*\"wells_fargo\" + 0.099*\"escrow\" + 0.046*\"tax\" + 0.045*\"taxis\" + 0.044*\"mortgage\" + 0.028*\"increase\" + 0.019*\"refund\" + 0.016*\"request\" + 0.016*\"would\" + 0.015*\"county\" + 0.014*\"year\" + 0.013*\"seterus\" + 0.013*\"banker\" + 0.012*\"irs\" + 0.012*\"freedom_mortgage\"'),\n",
       " (7,\n",
       "  '0.174*\"transunion\" + 0.065*\"citi\" + 0.044*\"lease\" + 0.026*\"apartment\" + 0.024*\"move\" + 0.023*\"information\" + 0.022*\"advise\" + 0.021*\"dispute\" + 0.021*\"verify\" + 0.019*\"back\" + 0.018*\"public_record\" + 0.018*\"send\" + 0.017*\"rent\" + 0.014*\"court\" + 0.013*\"contact\"'),\n",
       " (8,\n",
       "  '0.076*\"charge\" + 0.030*\"card\" + 0.029*\"dispute\" + 0.027*\"transaction\" + 0.023*\"bank_america\" + 0.023*\"claim\" + 0.022*\"purchase\" + 0.021*\"fraud\" + 0.016*\"make\" + 0.015*\"refund\" + 0.015*\"merchant\" + 0.014*\"cancel\" + 0.014*\"use\" + 0.012*\"return\" + 0.011*\"order\"'),\n",
       " (9,\n",
       "  '0.106*\"letter\" + 0.099*\"send\" + 0.051*\"receive\" + 0.031*\"request\" + 0.031*\"state\" + 0.027*\"document\" + 0.024*\"copy\" + 0.024*\"proof\" + 0.021*\"ask\" + 0.020*\"mail\" + 0.020*\"company\" + 0.018*\"provide\" + 0.016*\"response\" + 0.014*\"respond\" + 0.012*\"attach\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.print_topics(number_of_topics, num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             lowercase=True,\n",
    "                             norm='l2',\n",
    "                             max_df=.9,\n",
    "                             min_df=.1)\n",
    "tfidf_text = tfidf_text_vectorizer.fit_transform(train['COMPLAINT_TEXT'])\n",
    "text_cols = tfidf_text_vectorizer.get_feature_names()\n",
    "tfidf_text = pd.DataFrame(tfidf_text.todense(),\n",
    "                          columns=[text_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model with TF-IDF and Cosine Similarity in order to establish a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = [[entry[1] for entry in lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)] for i in range(len(train_text))]\n",
    "train_vecs = pd.DataFrame(columns=[f'topic_{i}' for i in range(number_of_topics)], data=top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train.reset_index(drop=True), train_vecs, tfidf_text], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>...</th>\n",
       "      <th>(xx xx,)</th>\n",
       "      <th>(xx xxxx,)</th>\n",
       "      <th>(xxxx,)</th>\n",
       "      <th>(xxxx and,)</th>\n",
       "      <th>(xxxx the,)</th>\n",
       "      <th>(xxxx to,)</th>\n",
       "      <th>(xxxx xxxx,)</th>\n",
       "      <th>(years,)</th>\n",
       "      <th>(you,)</th>\n",
       "      <th>(your,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have tried on several occasions to dispute a...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.206162</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135618</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>0.160793</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This complaint is in regards to overdraft fees...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.334402</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.328978</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071116</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My old car was repossessed after I became unem...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>someone else used my information to obtained c...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>0.806661</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Back in XX/XX/XXXX per Ace Cash Express I took...</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142890</td>\n",
       "      <td>0.185259</td>\n",
       "      <td>0.105884</td>\n",
       "      <td>0.06132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMPLAINT_TEXT  \\\n",
       "0  I have tried on several occasions to dispute a...   \n",
       "1  This complaint is in regards to overdraft fees...   \n",
       "2  My old car was repossessed after I became unem...   \n",
       "3  someone else used my information to obtained c...   \n",
       "4  Back in XX/XX/XXXX per Ace Cash Express I took...   \n",
       "\n",
       "                                        MAIN_PRODUCT       SUB_PRODUCT  \\\n",
       "0                                    Debt collection  Credit card debt   \n",
       "1                        Checking or savings account  Checking account   \n",
       "2  Credit reporting, credit repair services, or o...  Credit reporting   \n",
       "3  Credit reporting, credit repair services, or o...  Credit reporting   \n",
       "4          Payday loan, title loan, or personal loan  Installment loan   \n",
       "\n",
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0  0.000306  0.000306  0.000306  0.000306  0.008327  0.206162  0.000306   \n",
       "1  0.000333  0.000333  0.334402  0.000333  0.328978  0.000333  0.000333   \n",
       "2  0.001236  0.001236  0.001236  0.001236  0.001236  0.001236  0.001236   \n",
       "3  0.806661  0.006667  0.006667  0.006667  0.006667  0.006667  0.006667   \n",
       "4  0.035917  0.000630  0.000630  0.000630  0.000630  0.000630  0.000630   \n",
       "\n",
       "    ...     (xx xx,)  (xx xxxx,)   (xxxx,)  (xxxx and,)  (xxxx the,)  \\\n",
       "0   ...     0.135618    0.035166  0.160793      0.00000          0.0   \n",
       "1   ...     0.000000    0.000000  0.071116      0.00000          0.0   \n",
       "2   ...     0.000000    0.000000  0.000000      0.00000          0.0   \n",
       "3   ...     0.000000    0.000000  0.000000      0.00000          0.0   \n",
       "4   ...     0.142890    0.185259  0.105884      0.06132          0.0   \n",
       "\n",
       "   (xxxx to,)  (xxxx xxxx,)  (years,)    (you,)  (your,)  \n",
       "0         0.0      0.051132  0.000000  0.043348      0.0  \n",
       "1         0.0      0.030153  0.000000  0.000000      0.0  \n",
       "2         0.0      0.000000  0.000000  0.000000      0.0  \n",
       "3         0.0      0.000000  0.000000  0.000000      0.0  \n",
       "4         0.0      0.000000  0.089206  0.000000      0.0  \n",
       "\n",
       "[5 rows x 259 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final main-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to all classes to have the same number of data points\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(train_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1), train_df['MAIN_PRODUCT'])\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=40)\n",
    "X_train_scale = pca.fit_transform(X_train_scale)\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['COMPLAINT_TEXT'].tolist()\n",
    "test_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in test_text]\n",
    "test_text = [doc.lower().replace(r'\\n', '',) for doc in test_text]\n",
    "test_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_test = tfidf_text_vectorizer.transform(test['COMPLAINT_TEXT'])\n",
    "tfidf_text_test = pd.DataFrame(tfidf_text_test.todense(),\n",
    "                               columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(texts):\n",
    "    \"\"\"\n",
    "    For the test data we only need the bigram data built on train data,\n",
    "    as we'll use the train id2word mappings. This is a requirement due to \n",
    "    the shapes Gensim functions expect in the test-vector transformation below.\n",
    "    With both these in hand, we can make the test corpus.\n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram\n",
    "  \n",
    "bigram_test = get_bigram(test_text)\n",
    "\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
    "\n",
    "top_topics = [[entry[1] for entry in lda_train.get_document_topics(test_corpus[i], minimum_probability=0.0)] for i in range(len(test_text))]\n",
    "test_vecs = pd.DataFrame(columns=[f'topic_{i}' for i in range(number_of_topics)], data=top_topics)\n",
    "\n",
    "test_df = pd.concat([test.reset_index(drop=True), test_vecs, tfidf_text_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_test = np.array(test_df['MAIN_PRODUCT'])\n",
    "\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "X_test_scale = pca.transform(X_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7706026540444595\n",
      "Precision Score: 0.5900168731911648\n",
      "Recall Score: 0.5874451146753336\n",
      "F1 Score: 0.5811180389068358\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d4f681291268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilabel_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True Negative:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'False Positive:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "y_pred = random_forest.predict(X_test_scale)\n",
    "y_true = y_test\n",
    "\n",
    "print('Accuracy Score:', accuracy_score(y_true, y_pred))\n",
    "print('Precision Score:', precision_score(y_true, y_pred, average='macro'))\n",
    "print('Recall Score:', recall_score(y_true, y_pred, average='macro'))\n",
    "print('F1 Score:', f1_score(y_true, y_pred, average='macro'))\n",
    "print()\n",
    "conf_mat = multilabel_confusion_matrix(y_true, y_pred).ravel()\n",
    "np.array([[sum([item[0][0] for item in conf_mat]),\n",
    "           sum([item[0][1] for item in conf_mat])],\n",
    "          [sum([item[1][0] for item in conf_mat]),\n",
    "           sum([item[1][1] for item in conf_mat])]])\n",
    "print('True Negative:', tn)\n",
    "print('False Positive:', fp)\n",
    "print('False Negative:', fn)\n",
    "print('True Positive:', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10846,   305],\n",
       "        [  145,   535]],\n",
       "\n",
       "       [[10117,   385],\n",
       "        [  395,   934]],\n",
       "\n",
       "       [[ 5705,   569],\n",
       "        [  779,  4778]],\n",
       "\n",
       "       [[ 9073,   521],\n",
       "        [  568,  1669]],\n",
       "\n",
       "       [[11704,    33],\n",
       "        [   56,    38]],\n",
       "\n",
       "       [[10613,   315],\n",
       "        [  266,   637]],\n",
       "\n",
       "       [[11699,    25],\n",
       "        [   99,     8]],\n",
       "\n",
       "       [[10903,   321],\n",
       "        [  191,   416]],\n",
       "\n",
       "       [[11274,   240],\n",
       "        [  215,   102]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91934,  2714],\n",
       "       [ 2714,  9117]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[sum([item[0][0] for item in multilabel_confusion_matrix(y_true, y_pred)]),\n",
    "           sum([item[0][1] for item in multilabel_confusion_matrix(y_true, y_pred)])],\n",
    "          [sum([item[1][0] for item in multilabel_confusion_matrix(y_true, y_pred)]),\n",
    "           sum([item[1][1] for item in multilabel_confusion_matrix(y_true, y_pred)])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat = random_forest.feature_importances_\n",
    "# for i in range(number_of_topics):\n",
    "#     print('Topic', i, feat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final sub-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1)\n",
    "y_train = train_df['SUB_PRODUCT']\n",
    "\n",
    "main_product = train_df['MAIN_PRODUCT']\n",
    "labels = LabelEncoder()\n",
    "z = labels.fit_transform(main_product)\n",
    "\n",
    "X_train = np.concatenate((X_train, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "# Apply SMOTE to all classes to have the same number of data points\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)\n",
    "\n",
    "# Scale Data\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scale = scaler_2.fit_transform(X_train)\n",
    "\n",
    "# Apply PCA\n",
    "pca_2 = PCA(n_components=40)\n",
    "X_train_scale = pca_2.fit_transform(X_train_scale)\n",
    "\n",
    "random_forest_2 = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_test = np.array(test_df['SUB_PRODUCT'])\n",
    "\n",
    "main_product = random_forest.predict(X_test)\n",
    "z = labels.transform(main_product)\n",
    "X_test = np.concatenate((X_test, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "X_test_scale = scaler_2.transform(X_test)\n",
    "X_test_scale = pca_2.transform(X_test_scale)\n",
    "\n",
    "y_pred = random_forest_2.predict(X_test_scale)\n",
    "y_true = y_test\n",
    "\n",
    "print('Accuracy Score:', accuracy_score(y_true, y_pred))\n",
    "print('Precision Score:', precision_score(y_true, y_pred))\n",
    "print('Recall Score:', recall_score(y_true, y_pred))\n",
    "print('F1 Score:', f1_score(y_true, y_pred))\n",
    "print()\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print('True Negative:', tn)\n",
    "print('False Positive:', fp)\n",
    "print('False Negative:', fn)\n",
    "print('True Positive:', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = random_forest_2.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])\n",
    "print('Main-product prediction', feat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = defaultdict(int)\n",
    "for prod in test['MAIN_PRODUCT']:\n",
    "    d1[prod] += 1\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = defaultdict(int)\n",
    "for pred in random_forest.predict(np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))):\n",
    "    d2[pred] += 1\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = defaultdict(int)\n",
    "for prod in test['SUB_PRODUCT']:\n",
    "    d3[prod] += 1\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = defaultdict(int)\n",
    "for pred in random_forest_2.predict(X_test):\n",
    "    d4[pred] += 1\n",
    "d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
