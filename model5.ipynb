{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the table `compaints_users` for the complaint text and product ID, and the `products` table to find the product and sub-product\n",
    "df = pd.read_csv('data/complaints_users.csv')\n",
    "df2 = pd.read_csv('data/products.csv')\n",
    "df = df.merge(df2, how='left', on='PRODUCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sub-product of \"I do not know\" to null\n",
    "df.loc[df['SUB_PRODUCT'] == 'I do not know', 'SUB_PRODUCT'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs for which there have been no recent complaints\n",
    "df['date_norm'] = df['DATE'].apply(lambda x: int(x[6:10] + x[0:2] + x[3:5]))\n",
    "df = df[df['date_norm'] > 20180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complaints which have null values in either main or sub products\n",
    "df = df[(df['MAIN_PRODUCT'].notnull()) & (df['SUB_PRODUCT'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs which have fewer than 1000 complaints\n",
    "df['COMPLAINT_COUNTS'] = df.groupby('SUB_PRODUCT')['COMPLAINT_ID'].transform('count')\n",
    "df = df[df['COMPLAINT_COUNTS'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118302 total rows\n",
      "9 unique main-products\n",
      "17 unique sub-products\n"
     ]
    }
   ],
   "source": [
    "print(len(df), 'total rows')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 1000]['MAIN_PRODUCT'].unique()), 'unique main-products')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 1000]['SUB_PRODUCT'].unique()), 'unique sub-products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "    54735 Credit reporting\n",
      "    1030 Other personal consumer report\n",
      "    1 Conventional home mortgage\n",
      "\n",
      "Debt collection\n",
      "    4964 Medical debt\n",
      "    8401 Other debt\n",
      "    6799 Credit card debt\n",
      "    1123 Payday loan debt\n",
      "    1095 Auto debt\n",
      "\n",
      "Student loan\n",
      "    3956 Federal student loan servicing\n",
      "    2197 Private student loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "    2403 Store credit card\n",
      "    10359 General-purpose credit card or charge card\n",
      "\n",
      "Mortgage\n",
      "    6947 Conventional home mortgage\n",
      "    2288 FHA mortgage\n",
      "\n",
      "Checking or savings account\n",
      "    6745 Checking account\n",
      "\n",
      "Money transfer, virtual currency, or money service\n",
      "    1129 Domestic (US) money transfer\n",
      "\n",
      "Vehicle loan or lease\n",
      "    3024 Loan\n",
      "\n",
      "Payday loan, title loan, or personal loan\n",
      "    1106 Installment loan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "#     print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "#     print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print('   ', len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove that possibly mis-classified \"Convential home mortgage\" in \"Credit reporting...\" main-product\n",
    "df = (df[((df['MAIN_PRODUCT'] != 'Credit reporting, credit repair services, or other personal consumer reports') |\n",
    "     (df['SUB_PRODUCT'] != 'Conventional home mortgage'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118301"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "    54735 Credit reporting\n",
      "    1030 Other personal consumer report\n",
      "\n",
      "Debt collection\n",
      "    4964 Medical debt\n",
      "    8401 Other debt\n",
      "    6799 Credit card debt\n",
      "    1123 Payday loan debt\n",
      "    1095 Auto debt\n",
      "\n",
      "Student loan\n",
      "    3956 Federal student loan servicing\n",
      "    2197 Private student loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "    2403 Store credit card\n",
      "    10359 General-purpose credit card or charge card\n",
      "\n",
      "Mortgage\n",
      "    6947 Conventional home mortgage\n",
      "    2288 FHA mortgage\n",
      "\n",
      "Checking or savings account\n",
      "    6745 Checking account\n",
      "\n",
      "Money transfer, virtual currency, or money service\n",
      "    1129 Domestic (US) money transfer\n",
      "\n",
      "Vehicle loan or lease\n",
      "    3024 Loan\n",
      "\n",
      "Payday loan, title loan, or personal loan\n",
      "    1106 Installment loan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "#     print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "#     print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print('   ', len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT']], test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106470"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11831"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110606</th>\n",
       "      <td>Creditor : XXXX XXXX  Credit Bureau : Equifax ...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118824</th>\n",
       "      <td>I received a SBA sponsored loan from Regions B...</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15974</th>\n",
       "      <td>Inquiry XXXX I received a promotional offer to...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10573</th>\n",
       "      <td>When creditors pull my credit from Equifax ( o...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54104</th>\n",
       "      <td>HSBC advertised that I would receive {$150.00}...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           COMPLAINT_TEXT  \\\n",
       "110606  Creditor : XXXX XXXX  Credit Bureau : Equifax ...   \n",
       "118824  I received a SBA sponsored loan from Regions B...   \n",
       "15974   Inquiry XXXX I received a promotional offer to...   \n",
       "10573   When creditors pull my credit from Equifax ( o...   \n",
       "54104   HSBC advertised that I would receive {$150.00}...   \n",
       "\n",
       "                                             MAIN_PRODUCT  \\\n",
       "110606  Credit reporting, credit repair services, or o...   \n",
       "118824          Payday loan, title loan, or personal loan   \n",
       "15974                         Checking or savings account   \n",
       "10573   Credit reporting, credit repair services, or o...   \n",
       "54104                         Credit card or prepaid card   \n",
       "\n",
       "                                       SUB_PRODUCT  \n",
       "110606                            Credit reporting  \n",
       "118824                            Installment loan  \n",
       "15974                             Checking account  \n",
       "10573                             Credit reporting  \n",
       "54104   General-purpose credit card or charge card  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['COMPLAINT_TEXT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "train_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [doc.lower().replace(r'\\n', '',) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count=bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "def get_corpus(texts):\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 30\n",
    "\n",
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus,\n",
    "                           num_topics=number_of_topics,\n",
    "                           id2word=train_id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train.print_topics(number_of_topics, num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             lowercase=True,\n",
    "                             norm='l2',\n",
    "                             max_df=.9,\n",
    "                             min_df=.1)\n",
    "tfidf_text = tfidf_text_vectorizer.fit_transform(train['COMPLAINT_TEXT'])\n",
    "text_cols = tfidf_text_vectorizer.get_feature_names()\n",
    "tfidf_text = pd.DataFrame(tfidf_text.todense(),\n",
    "                          columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = [[entry[1] for entry in lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)] for i in range(len(train_text))]\n",
    "train_vecs = pd.DataFrame(columns=[f'topic_{i}' for i in range(number_of_topics)], data=top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train.reset_index(drop=True), train_vecs, tfidf_text], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final main-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to all classes to have the same number of data points\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(train_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1), train_df['MAIN_PRODUCT'])\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['COMPLAINT_TEXT'].tolist()\n",
    "test_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in test_text]\n",
    "test_text = [doc.lower().replace(r'\\n', '',) for doc in test_text]\n",
    "test_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_test = tfidf_text_vectorizer.transform(test['COMPLAINT_TEXT'])\n",
    "tfidf_text_test = pd.DataFrame(tfidf_text_test.todense(),\n",
    "                               columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(texts):\n",
    "    \"\"\"\n",
    "    For the test data we only need the bigram data built on train data,\n",
    "    as we'll use the train id2word mappings. This is a requirement due to \n",
    "    the shapes Gensim functions expect in the test-vector transformation below.\n",
    "    With both these in hand, we can make the test corpus.\n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram\n",
    "  \n",
    "bigram_test = get_bigram(test_text)\n",
    "\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
    "\n",
    "top_topics = [[entry[1] for entry in lda_train.get_document_topics(test_corpus[i], minimum_probability=0.0)] for i in range(len(test_text))]\n",
    "test_vecs = pd.DataFrame(columns=[f'topic_{i}' for i in range(number_of_topics)], data=top_topics)\n",
    "\n",
    "test_df = pd.concat([test.reset_index(drop=True), test_vecs, tfidf_text_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_test = np.array(test_df['MAIN_PRODUCT'])\n",
    "\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test f1: 0.671 +/- 0.251\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_forest.predict(X_test_scale)\n",
    "rf_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Random Forest Test f1: {np.mean(rf_f1_test):.3f} +/- {np.std(rf_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 0.06349135381860481\n",
      "Topic 1 0.04446204479209733\n",
      "Topic 2 0.0038677444933119796\n",
      "Topic 3 0.004084370873210003\n",
      "Topic 4 0.041428115121806414\n",
      "Topic 5 0.005960204363099697\n",
      "Topic 6 0.02993647180911589\n",
      "Topic 7 0.007412270575778321\n",
      "Topic 8 0.006871983311316441\n",
      "Topic 9 0.024182480601425225\n",
      "Topic 10 0.005565830695858488\n",
      "Topic 11 0.008672906119939986\n",
      "Topic 12 0.004561762223484695\n",
      "Topic 13 0.007434285442441586\n",
      "Topic 14 0.01095607964899817\n",
      "Topic 15 0.003906914921145605\n",
      "Topic 16 0.031368869376899526\n",
      "Topic 17 0.013385961761272897\n",
      "Topic 18 0.0077889823786315366\n",
      "Topic 19 0.0037384395929705447\n"
     ]
    }
   ],
   "source": [
    "feat = random_forest.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final sub-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test f1: 0.256 +/- 0.207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1)\n",
    "y_train = train_df['SUB_PRODUCT']\n",
    "\n",
    "main_product = train_df['MAIN_PRODUCT']\n",
    "labels = LabelEncoder()\n",
    "z = labels.fit_transform(main_product)\n",
    "\n",
    "X_train = np.concatenate((X_train, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "# Apply SMOTE to all classes to have the same number of data points\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)\n",
    "\n",
    "# Scale Data\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scale = scaler_2.fit_transform(X_train)\n",
    "\n",
    "random_forest_2 = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_test = np.array(test_df['SUB_PRODUCT'])\n",
    "\n",
    "main_product = random_forest.predict(X_test)\n",
    "z = labels.transform(main_product)\n",
    "X_test = np.concatenate((X_test, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "X_test_scale = scaler_2.transform(X_test)\n",
    "\n",
    "y_pred = random_forest_2.predict(X_test_scale)\n",
    "rf_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Random Forest Test f1: {np.mean(rf_f1_test):.3f} +/- {np.std(rf_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 0.033247422669671005\n",
      "Topic 1 0.024860140095903375\n",
      "Topic 2 0.004242565963317391\n",
      "Topic 3 0.004526497225359549\n",
      "Topic 4 0.022885324016831653\n",
      "Topic 5 0.0059284363197458214\n",
      "Topic 6 0.018833586285517442\n",
      "Topic 7 0.006290588015848359\n",
      "Topic 8 0.005976243270436246\n",
      "Topic 9 0.018765337590924625\n",
      "Topic 10 0.006907409470613672\n",
      "Topic 11 0.009211821451109502\n",
      "Topic 12 0.004662545837562532\n",
      "Topic 13 0.009516267064659007\n",
      "Topic 14 0.006190658979237472\n",
      "Topic 15 0.005371819137947893\n",
      "Topic 16 0.018730482340298442\n",
      "Topic 17 0.010487395600459182\n",
      "Topic 18 0.007662127998519065\n",
      "Topic 19 0.003950319143307471\n",
      "Main-product prediction 0.13067493181555678\n"
     ]
    }
   ],
   "source": [
    "feat = random_forest_2.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])\n",
    "print('Main-product prediction', feat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit reporting, credit repair services, or other personal consumer reports': 5557,\n",
       "             'Debt collection': 2249,\n",
       "             'Mortgage': 944,\n",
       "             'Credit card or prepaid card': 1311,\n",
       "             'Student loan': 608,\n",
       "             'Payday loan, title loan, or personal loan': 94,\n",
       "             'Vehicle loan or lease': 315,\n",
       "             'Checking or savings account': 643,\n",
       "             'Money transfer, virtual currency, or money service': 110})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = defaultdict(int)\n",
    "for prod in test['MAIN_PRODUCT']:\n",
    "    d1[prod] += 1\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Vehicle loan or lease': 10272,\n",
       "             'Debt collection': 758,\n",
       "             'Credit card or prepaid card': 549,\n",
       "             'Student loan': 149,\n",
       "             'Mortgage': 103})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = defaultdict(int)\n",
    "for pred in random_forest.predict(np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))):\n",
    "    d2[pred] += 1\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit reporting': 5458,\n",
       "             'Credit card debt': 648,\n",
       "             'FHA mortgage': 222,\n",
       "             'General-purpose credit card or charge card': 1069,\n",
       "             'Federal student loan servicing': 384,\n",
       "             'Medical debt': 522,\n",
       "             'Conventional home mortgage': 722,\n",
       "             'Other debt': 861,\n",
       "             'Installment loan': 94,\n",
       "             'Loan': 315,\n",
       "             'Checking account': 643,\n",
       "             'Private student loan': 224,\n",
       "             'Domestic (US) money transfer': 110,\n",
       "             'Other personal consumer report': 99,\n",
       "             'Payday loan debt': 114,\n",
       "             'Store credit card': 242,\n",
       "             'Auto debt': 104})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = defaultdict(int)\n",
    "for prod in test['SUB_PRODUCT']:\n",
    "    d3[prod] += 1\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Loan': 11273,\n",
       "             'Credit card debt': 549,\n",
       "             'Private student loan': 9})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4 = defaultdict(int)\n",
    "for pred in random_forest_2.predict(X_test):\n",
    "    d4[pred] += 1\n",
    "d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
