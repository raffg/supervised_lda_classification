{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the table `compaints_users` for the complaint text and product ID, and the `products` table to find the product and sub-product\n",
    "df = pd.read_csv('data/complaints_users.csv')\n",
    "df2 = pd.read_csv('data/products.csv')\n",
    "df = df.merge(df2, how='left', on='PRODUCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sub-product of \"I do not know\" to null\n",
    "df.loc[df['SUB_PRODUCT'] == 'I do not know', 'SUB_PRODUCT'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs for which there have been no recent complaints\n",
    "df['date_norm'] = df['DATE'].apply(lambda x: int(x[6:10] + x[0:2] + x[3:5]))\n",
    "df = df[df['date_norm'] > 20180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complaints which have null values in either main or sub products\n",
    "df = df[(df['MAIN_PRODUCT'].notnull()) & (df['SUB_PRODUCT'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs which have fewer than 1000 complaints\n",
    "df['COMPLAINT_COUNTS'] = df.groupby('SUB_PRODUCT')['COMPLAINT_ID'].transform('count')\n",
    "df = df[df['COMPLAINT_COUNTS'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118302 total rows\n",
      "9 unique main-products\n",
      "17 unique sub-products\n"
     ]
    }
   ],
   "source": [
    "print(len(df), 'total rows')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 1000]['MAIN_PRODUCT'].unique()), 'unique main-products')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 1000]['SUB_PRODUCT'].unique()), 'unique sub-products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "3\n",
      "['Credit reporting' 'Other personal consumer report'\n",
      " 'Conventional home mortgage']\n",
      "54735 Credit reporting\n",
      "1030 Other personal consumer report\n",
      "1 Conventional home mortgage\n",
      "\n",
      "Debt collection\n",
      "5\n",
      "['Medical debt' 'Other debt' 'Credit card debt' 'Payday loan debt'\n",
      " 'Auto debt']\n",
      "4964 Medical debt\n",
      "8401 Other debt\n",
      "6799 Credit card debt\n",
      "1123 Payday loan debt\n",
      "1095 Auto debt\n",
      "\n",
      "Student loan\n",
      "2\n",
      "['Federal student loan servicing' 'Private student loan']\n",
      "3956 Federal student loan servicing\n",
      "2197 Private student loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "2\n",
      "['Store credit card' 'General-purpose credit card or charge card']\n",
      "2403 Store credit card\n",
      "10359 General-purpose credit card or charge card\n",
      "\n",
      "Mortgage\n",
      "2\n",
      "['Conventional home mortgage' 'FHA mortgage']\n",
      "6947 Conventional home mortgage\n",
      "2288 FHA mortgage\n",
      "\n",
      "Checking or savings account\n",
      "1\n",
      "['Checking account']\n",
      "6745 Checking account\n",
      "\n",
      "Money transfer, virtual currency, or money service\n",
      "1\n",
      "['Domestic (US) money transfer']\n",
      "1129 Domestic (US) money transfer\n",
      "\n",
      "Vehicle loan or lease\n",
      "1\n",
      "['Loan']\n",
      "3024 Loan\n",
      "\n",
      "Payday loan, title loan, or personal loan\n",
      "1\n",
      "['Installment loan']\n",
      "1106 Installment loan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "    print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "    print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print(len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove that possibly mis-classified \"Convential home mortgage\" in \"Credit reporting...\" main-product\n",
    "df = (df[((df['MAIN_PRODUCT'] != 'Credit reporting, credit repair services, or other personal consumer reports') |\n",
    "     (df['SUB_PRODUCT'] != 'Conventional home mortgage'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(df, column, sample_size):\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for group in df[column].unique():\n",
    "        sample = sample_size\n",
    "        data = len(df[df[column] == group])\n",
    "        if data <= sample:\n",
    "            new_df = new_df.append(df[df[column] == group])\n",
    "            sample -= data\n",
    "        new_df = new_df.append(resample(df[df[column] == group], n_samples=sample))\n",
    "    return new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201656"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "1\n",
      "['Credit reporting']\n",
      "89994 Credit reporting\n",
      "\n",
      "Student loan\n",
      "1\n",
      "['Federal student loan servicing']\n",
      "12138 Federal student loan servicing\n",
      "\n",
      "Debt collection\n",
      "3\n",
      "['Other debt' 'Credit card debt' 'Other (i.e. phone, health club, etc.)']\n",
      "13385 Other debt\n",
      "10021 Credit card debt\n",
      "12393 Other (i.e. phone, health club, etc.)\n",
      "\n",
      "Mortgage\n",
      "2\n",
      "['Conventional home mortgage' 'Conventional fixed mortgage']\n",
      "12098 Conventional home mortgage\n",
      "14562 Conventional fixed mortgage\n",
      "\n",
      "Checking or savings account\n",
      "1\n",
      "['Checking account']\n",
      "10709 Checking account\n",
      "\n",
      "Credit card or prepaid card\n",
      "1\n",
      "['General-purpose credit card or charge card']\n",
      "16243 General-purpose credit card or charge card\n",
      "\n",
      "Bank account or service\n",
      "1\n",
      "['Checking account']\n",
      "10113 Checking account\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "    print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "    print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print(len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT']], test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106470"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11831"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98771</th>\n",
       "      <td>Because my husband passed away onXX/XX/XXXX an...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94914</th>\n",
       "      <td>XXXX XXXX XXXX XXXX stated that I owe Student ...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15523</th>\n",
       "      <td>I applied online and I was approved for {$970....</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130459</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27717</th>\n",
       "      <td>Back in XXXX XXXX, I was contacted by a Law Fi...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card debt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           COMPLAINT_TEXT  \\\n",
       "98771   Because my husband passed away onXX/XX/XXXX an...   \n",
       "94914   XXXX XXXX XXXX XXXX stated that I owe Student ...   \n",
       "15523   I applied online and I was approved for {$970....   \n",
       "130459  There are many mistakes appear in my report wi...   \n",
       "27717   Back in XXXX XXXX, I was contacted by a Law Fi...   \n",
       "\n",
       "                                             MAIN_PRODUCT  \\\n",
       "98771                         Credit card or prepaid card   \n",
       "94914   Credit reporting, credit repair services, or o...   \n",
       "15523           Payday loan, title loan, or personal loan   \n",
       "130459  Credit reporting, credit repair services, or o...   \n",
       "27717                                     Debt collection   \n",
       "\n",
       "                                       SUB_PRODUCT  \n",
       "98771   General-purpose credit card or charge card  \n",
       "94914                             Credit reporting  \n",
       "15523                             Installment loan  \n",
       "130459                            Credit reporting  \n",
       "27717                             Credit card debt  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['COMPLAINT_TEXT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "train_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [doc.lower().replace(r'\\n', '',) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count=bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "def get_corpus(texts):\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 20\n",
    "\n",
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus,\n",
    "                           num_topics=number_of_topics,\n",
    "                           id2word=train_id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.043*\"experian\" + 0.036*\"verify\" + 0.026*\"dispute\" + 0.025*\"delete\" + 0.021*\"item\" + 0.020*\"fcra\" + 0.020*\"please\" + 0.019*\"provide\" + 0.019*\"information\" + 0.016*\"remove\" + 0.015*\"request\" + 0.015*\"section\" + 0.014*\"file\" + 0.014*\"law\" + 0.013*\"reporting_act\"'),\n",
       " (1,\n",
       "  '0.390*\"bank\" + 0.222*\"chase\" + 0.086*\"us\" + 0.010*\"usa\" + 0.010*\"customer\" + 0.007*\"freedom\" + 0.006*\"jp_morgan\" + 0.006*\"enhanced_recovery\" + 0.005*\"banks\" + 0.005*\"banking\" + 0.005*\"united\" + 0.005*\"banker\" + 0.004*\"department\" + 0.004*\"claim\" + 0.004*\"branch\"'),\n",
       " (2,\n",
       "  '0.153*\"payment\" + 0.048*\"late\" + 0.042*\"pay\" + 0.038*\"make\" + 0.027*\"month\" + 0.018*\"amount\" + 0.018*\"due\" + 0.017*\"day\" + 0.016*\"balance\" + 0.015*\"fee\" + 0.013*\"statement\" + 0.013*\"time\" + 0.012*\"monthly\" + 0.011*\"interest\" + 0.011*\"show\"'),\n",
       " (3,\n",
       "  '0.089*\"letter\" + 0.081*\"send\" + 0.062*\"receive\" + 0.040*\"request\" + 0.026*\"state\" + 0.024*\"mail\" + 0.021*\"copy\" + 0.021*\"document\" + 0.017*\"response\" + 0.016*\"provide\" + 0.015*\"information\" + 0.014*\"email\" + 0.014*\"date\" + 0.013*\"ask\" + 0.013*\"day\"'),\n",
       " (4,\n",
       "  '0.200*\"open\" + 0.128*\"close\" + 0.124*\"balance\" + 0.086*\"date\" + 0.042*\"citibank\" + 0.038*\"status\" + 0.031*\"closed\" + 0.025*\"show\" + 0.016*\"last\" + 0.015*\"type\" + 0.014*\"new\" + 0.011*\"still\" + 0.011*\"high\" + 0.011*\"name\" + 0.011*\"transfer\"'),\n",
       " (5,\n",
       "  '0.055*\"information\" + 0.046*\"remove\" + 0.045*\"dispute\" + 0.041*\"equifax\" + 0.023*\"bureaus\" + 0.022*\"address\" + 0.020*\"file\" + 0.016*\"show\" + 0.016*\"update\" + 0.015*\"correct\" + 0.014*\"transunion\" + 0.013*\"contact\" + 0.013*\"incorrect\" + 0.013*\"bureau\" + 0.013*\"also\"'),\n",
       " (6,\n",
       "  '0.053*\"check\" + 0.053*\"bank\" + 0.028*\"money\" + 0.023*\"fee\" + 0.023*\"fund\" + 0.022*\"deposit\" + 0.019*\"bank_america\" + 0.019*\"charge\" + 0.016*\"day\" + 0.016*\"checking\" + 0.016*\"transaction\" + 0.013*\"wells_fargo\" + 0.011*\"make\" + 0.011*\"cash\" + 0.010*\"branch\"'),\n",
       " (7,\n",
       "  '0.019*\"complaint\" + 0.012*\"provide\" + 0.009*\"cfpb\" + 0.008*\"issue\" + 0.007*\"matter\" + 0.007*\"fact\" + 0.007*\"claim\" + 0.006*\"action\" + 0.006*\"consumer\" + 0.006*\"make\" + 0.006*\"state\" + 0.006*\"case\" + 0.005*\"record\" + 0.005*\"right\" + 0.005*\"attempt\"'),\n",
       " (8,\n",
       "  '0.127*\"debt\" + 0.032*\"collection\" + 0.027*\"company\" + 0.021*\"validation\" + 0.019*\"provide\" + 0.018*\"contract\" + 0.016*\"dispute\" + 0.015*\"owe\" + 0.014*\"request\" + 0.013*\"collect\" + 0.013*\"validate\" + 0.012*\"proof\" + 0.012*\"collection_agency\" + 0.011*\"state\" + 0.010*\"original_creditor\"'),\n",
       " (9,\n",
       "  '0.168*\"card\" + 0.083*\"charge\" + 0.024*\"dispute\" + 0.023*\"purchase\" + 0.023*\"capital_one\" + 0.022*\"use\" + 0.014*\"claim\" + 0.014*\"transaction\" + 0.012*\"make\" + 0.012*\"merchant\" + 0.012*\"discover\" + 0.011*\"limit\" + 0.011*\"never\" + 0.011*\"refund\" + 0.010*\"fraud\"')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.print_topics(number_of_topics, num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             lowercase=True,\n",
    "                             norm='l2',\n",
    "                             max_df=.9,\n",
    "                             min_df=.1)\n",
    "tfidf_text = tfidf_text_vectorizer.fit_transform(train['COMPLAINT_TEXT'])\n",
    "text_cols = tfidf_text_vectorizer.get_feature_names()\n",
    "tfidf_text = pd.DataFrame(tfidf_text.todense(),\n",
    "                          columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = [[entry[1] for entry in lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)] for i in range(len(train_text))]\n",
    "train_vecs = pd.DataFrame(columns=[f'topic_{i}' for i in range(number_of_topics)], data=top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train.reset_index(drop=True), train_vecs, tfidf_text], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>...</th>\n",
       "      <th>(xx xx,)</th>\n",
       "      <th>(xx xxxx,)</th>\n",
       "      <th>(xxxx,)</th>\n",
       "      <th>(xxxx and,)</th>\n",
       "      <th>(xxxx the,)</th>\n",
       "      <th>(xxxx to,)</th>\n",
       "      <th>(xxxx xxxx,)</th>\n",
       "      <th>(years,)</th>\n",
       "      <th>(you,)</th>\n",
       "      <th>(your,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It 's now been more than six months since we a...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.237565</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.092457</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.120338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P Morgan Chase allow criminals to steal {$2400...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.058841</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.181336</td>\n",
       "      <td>0.022928</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.176606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236106</td>\n",
       "      <td>0.279961</td>\n",
       "      <td>0.407657</td>\n",
       "      <td>0.065442</td>\n",
       "      <td>0.051346</td>\n",
       "      <td>0.032397</td>\n",
       "      <td>0.166251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi, I made a mortgage payment twice accidently...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Conventional home mortgage</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.106817</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.414505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171865</td>\n",
       "      <td>0.203788</td>\n",
       "      <td>0.137879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I faxed a letter to XXXX  last XXXX of XXXX to...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>0.212922</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.427657</td>\n",
       "      <td>0.031909</td>\n",
       "      <td>0.159266</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094112</td>\n",
       "      <td>0.055796</td>\n",
       "      <td>0.481319</td>\n",
       "      <td>0.049996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074253</td>\n",
       "      <td>0.326604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XX/XX/2017 REPORTING AN INQUIRY ON MY REPORT</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMPLAINT_TEXT  \\\n",
       "0  It 's now been more than six months since we a...   \n",
       "1  P Morgan Chase allow criminals to steal {$2400...   \n",
       "2  Hi, I made a mortgage payment twice accidently...   \n",
       "3  I faxed a letter to XXXX  last XXXX of XXXX to...   \n",
       "4       XX/XX/2017 REPORTING AN INQUIRY ON MY REPORT   \n",
       "\n",
       "                                        MAIN_PRODUCT  \\\n",
       "0                        Credit card or prepaid card   \n",
       "1                        Checking or savings account   \n",
       "2                                           Mortgage   \n",
       "3  Credit reporting, credit repair services, or o...   \n",
       "4  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "                                  SUB_PRODUCT   topic_0   topic_1   topic_2  \\\n",
       "0  General-purpose credit card or charge card  0.001191  0.001191  0.237565   \n",
       "1                            Checking account  0.000162  0.058841  0.000162   \n",
       "2                  Conventional home mortgage  0.001042  0.001042  0.106817   \n",
       "3                            Credit reporting  0.212922  0.000944  0.000944   \n",
       "4                            Credit reporting  0.025000  0.025000  0.025000   \n",
       "\n",
       "    topic_3   topic_4   topic_5   topic_6   ...     (xx xx,)  (xx xxxx,)  \\\n",
       "0  0.001191  0.092457  0.001191  0.120338   ...     0.000000    0.000000   \n",
       "1  0.181336  0.022928  0.000162  0.176606   ...     0.236106    0.279961   \n",
       "2  0.001042  0.001042  0.001042  0.414505   ...     0.171865    0.203788   \n",
       "3  0.427657  0.031909  0.159266  0.000944   ...     0.094112    0.055796   \n",
       "4  0.025000  0.025000  0.025000  0.025000   ...     0.307839    0.000000   \n",
       "\n",
       "    (xxxx,)  (xxxx and,)  (xxxx the,)  (xxxx to,)  (xxxx xxxx,)  (years,)  \\\n",
       "0  0.037150     0.000000     0.000000    0.097430      0.000000       0.0   \n",
       "1  0.407657     0.065442     0.051346    0.032397      0.166251       0.0   \n",
       "2  0.137879     0.000000     0.000000    0.000000      0.000000       0.0   \n",
       "3  0.481319     0.049996     0.000000    0.074253      0.326604       0.0   \n",
       "4  0.000000     0.000000     0.000000    0.000000      0.000000       0.0   \n",
       "\n",
       "   (you,)  (your,)  \n",
       "0     0.0      0.0  \n",
       "1     0.0      0.0  \n",
       "2     0.0      0.0  \n",
       "3     0.0      0.0  \n",
       "4     0.0      0.0  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap / downsample all classes to have the same number of data points\n",
    "rebalanced = rebalance(X_train, 'MAIN_PRODUCT', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final main-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(rebalanced.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_train = np.array(rebalanced['MAIN_PRODUCT'])\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['COMPLAINT_TEXT'].tolist()\n",
    "test_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in test_text]\n",
    "test_text = [doc.lower().replace(r'\\n', '',) for doc in test_text]\n",
    "test_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_test = tfidf_text_vectorizer.transform(test['COMPLAINT_TEXT'])\n",
    "tfidf_text_test = pd.DataFrame(tfidf_text_test.todense(),\n",
    "                               columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(texts):\n",
    "    \"\"\"\n",
    "    For the test data we only need the bigram data built on train data,\n",
    "    as we'll use the train id2word mappings. This is a requirement due to \n",
    "    the shapes Gensim functions expect in the test-vector transformation below.\n",
    "    With both these in hand, we can make the test corpus.\n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram\n",
    "  \n",
    "bigram_test = get_bigram(test_text)\n",
    "\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
    "\n",
    "top_topics = [[entry[1] for entry in lda_train.get_document_topics(test_corpus[i], minimum_probability=0.0)] for i in range(len(test_text))]\n",
    "test_vecs = pd.DataFrame(columns=[f'topic_{i}' for i in range(number_of_topics)], data=top_topics)\n",
    "\n",
    "test_df = pd.concat([test.reset_index(drop=True), test_vecs, tfidf_text_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_test = np.array(test_df['MAIN_PRODUCT'])\n",
    "\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test f1: 0.752 +/- 0.110\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_forest.predict(X_test_scale)\n",
    "rf_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Random Forest Test f1: {np.mean(rf_f1_test):.3f} +/- {np.std(rf_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 0.012464334495036167\n",
      "Topic 1 0.006985890733496471\n",
      "Topic 2 0.011738427690271022\n",
      "Topic 3 0.00513387219436277\n",
      "Topic 4 0.008193774258503176\n",
      "Topic 5 0.020707538649739815\n",
      "Topic 6 0.05052860388473234\n",
      "Topic 7 0.0051188778882185855\n",
      "Topic 8 0.022637753824300475\n",
      "Topic 9 0.031071676819971375\n",
      "Topic 10 0.010547749866741462\n",
      "Topic 11 0.006217632441264317\n",
      "Topic 12 0.0051393136654502925\n",
      "Topic 13 0.012529654077762707\n",
      "Topic 14 0.0664739293002951\n",
      "Topic 15 0.007197774151005829\n",
      "Topic 16 0.021878988142118375\n",
      "Topic 17 0.06868823748886127\n",
      "Topic 18 0.006968779816198093\n",
      "Topic 19 0.005997506049956088\n"
     ]
    }
   ],
   "source": [
    "feat = random_forest.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final sub-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test f1: 0.501 +/- 0.202\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap / downsample all classes to have the same number of data points\n",
    "rebalanced = rebalance(train_df, 'SUB_PRODUCT', 10000)\n",
    "\n",
    "X_train = np.array(rebalanced.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_train = np.array(rebalanced['SUB_PRODUCT'])\n",
    "\n",
    "main_product = rebalanced['MAIN_PRODUCT']\n",
    "labels = LabelEncoder()\n",
    "z = labels.fit_transform(main_product)\n",
    "X_train = np.concatenate((X_train, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "# Scale Data\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scale = scaler_2.fit_transform(X_train)\n",
    "\n",
    "random_forest_2 = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))\n",
    "y_test = np.array(test_df['SUB_PRODUCT'])\n",
    "\n",
    "main_product = random_forest.predict(X_test)\n",
    "z = labels.transform(main_product)\n",
    "X_test = np.concatenate((X_test, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "X_test_scale = scaler_2.transform(X_test)\n",
    "\n",
    "y_pred = random_forest_2.predict(X_test_scale)\n",
    "rf_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Random Forest Test f1: {np.mean(rf_f1_test):.3f} +/- {np.std(rf_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 0.009675593424632185\n",
      "Topic 1 0.00610634626388659\n",
      "Topic 2 0.008771740969758296\n",
      "Topic 3 0.005612843386573118\n",
      "Topic 4 0.006966416104105308\n",
      "Topic 5 0.014392631912279822\n",
      "Topic 6 0.030159035435433634\n",
      "Topic 7 0.0053248368113883275\n",
      "Topic 8 0.01971917907964939\n",
      "Topic 9 0.024829578612542254\n",
      "Topic 10 0.008806374327569817\n",
      "Topic 11 0.006461892379303025\n",
      "Topic 12 0.005420422841277358\n",
      "Topic 13 0.010914258504141475\n",
      "Topic 14 0.03872405578001432\n",
      "Topic 15 0.007230260210116026\n",
      "Topic 16 0.021593228522847084\n",
      "Topic 17 0.04720126190228999\n",
      "Topic 18 0.00624641394554231\n",
      "Topic 19 0.007685876150186542\n",
      "Main-product prediction 0.16799294880169588\n"
     ]
    }
   ],
   "source": [
    "feat = random_forest_2.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])\n",
    "print('Main-product prediction', feat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit reporting, credit repair services, or other personal consumer reports': 8988,\n",
       "             'Credit card or prepaid card': 1658,\n",
       "             'Bank account or service': 998,\n",
       "             'Debt collection': 3588,\n",
       "             'Checking or savings account': 1100,\n",
       "             'Mortgage': 2614,\n",
       "             'Student loan': 1220})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = defaultdict(int)\n",
    "for prod in test['MAIN_PRODUCT']:\n",
    "    d1[prod] += 1\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit card or prepaid card': 15448,\n",
       "             'Credit reporting, credit repair services, or other personal consumer reports': 750,\n",
       "             'Checking or savings account': 1988,\n",
       "             'Debt collection': 339,\n",
       "             'Mortgage': 1597,\n",
       "             'Student loan': 44})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = defaultdict(int)\n",
    "for pred in random_forest.predict(np.array(test_df.drop(['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT'], axis=1))):\n",
    "    d2[pred] += 1\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = defaultdict(int)\n",
    "for prod in test['SUB_PRODUCT']:\n",
    "    d3[prod] += 1\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = defaultdict(int)\n",
    "for pred in random_forest_2.predict(X_test):\n",
    "    d4[pred] += 1\n",
    "d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
