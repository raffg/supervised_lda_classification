{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the table `compaints_users` for the complaint text and product ID, and the `products` table to find the product and sub-product\n",
    "df = pd.read_csv('data/complaints_users.csv')\n",
    "df2 = pd.read_csv('data/products.csv')\n",
    "df = df.merge(df2, how='left', on='PRODUCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sub-product of \"I do not know\" to null\n",
    "df.loc[df['SUB_PRODUCT'] == 'I do not know', 'SUB_PRODUCT'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove product IDs for which there have been no recent complaints\n",
    "# df['date_norm'] = df['DATE'].apply(lambda x: int(x[6:10] + x[0:2] + x[3:5]))\n",
    "# df = df[df['date_norm'] > 20180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complaints which have null values in either main or sub products\n",
    "df = df[(df['MAIN_PRODUCT'].notnull()) & (df['SUB_PRODUCT'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs which have fewer than 100 complaints\n",
    "df['COMPLAINT_COUNTS'] = df.groupby('SUB_PRODUCT')['COMPLAINT_ID'].transform('count')\n",
    "df = df[df['COMPLAINT_COUNTS'] > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112819 total rows\n",
      "7 unique main-products\n",
      "12 unique sub-products\n"
     ]
    }
   ],
   "source": [
    "print(len(df), 'total rows')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 2000]['MAIN_PRODUCT'].unique()), 'unique main-products')\n",
    "print(len(df[df['COMPLAINT_COUNTS'] > 2000]['SUB_PRODUCT'].unique()), 'unique sub-products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "2\n",
      "['Credit reporting' 'Conventional home mortgage']\n",
      "54735 Credit reporting\n",
      "1 Conventional home mortgage\n",
      "\n",
      "Debt collection\n",
      "3\n",
      "['Medical debt' 'Other debt' 'Credit card debt']\n",
      "4964 Medical debt\n",
      "8401 Other debt\n",
      "6799 Credit card debt\n",
      "\n",
      "Student loan\n",
      "2\n",
      "['Federal student loan servicing' 'Private student loan']\n",
      "3956 Federal student loan servicing\n",
      "2197 Private student loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "2\n",
      "['Store credit card' 'General-purpose credit card or charge card']\n",
      "2403 Store credit card\n",
      "10359 General-purpose credit card or charge card\n",
      "\n",
      "Mortgage\n",
      "2\n",
      "['Conventional home mortgage' 'FHA mortgage']\n",
      "6947 Conventional home mortgage\n",
      "2288 FHA mortgage\n",
      "\n",
      "Checking or savings account\n",
      "1\n",
      "['Checking account']\n",
      "6745 Checking account\n",
      "\n",
      "Vehicle loan or lease\n",
      "1\n",
      "['Loan']\n",
      "3024 Loan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "    print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "    print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print(len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove that possibly mis-classified \"Convential home mortgage\" in \"Credit reporting...\" main-product\n",
    "df = (df[((df['MAIN_PRODUCT'] != 'Credit reporting, credit repair services, or other personal consumer reports') |\n",
    "     (df['SUB_PRODUCT'] != 'Conventional home mortgage'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(df, column, sample_size):\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for group in df[column].unique():\n",
    "        sample = sample_size\n",
    "        data = len(df[df[column] == group])\n",
    "        if data <= sample:\n",
    "            new_df = new_df.append(df[df[column] == group])\n",
    "            sample -= data\n",
    "        new_df = new_df.append(resample(df[df[column] == group], n_samples=sample))\n",
    "    return new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap / downsample all classes to have the same number of data points\n",
    "df = rebalance(df, 'MAIN_PRODUCT', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking or savings account\n",
      "1\n",
      "['Checking account']\n",
      "10000 Checking account\n",
      "\n",
      "Vehicle loan or lease\n",
      "1\n",
      "['Loan']\n",
      "10000 Loan\n",
      "\n",
      "Mortgage\n",
      "2\n",
      "['FHA mortgage' 'Conventional home mortgage']\n",
      "2474 FHA mortgage\n",
      "7526 Conventional home mortgage\n",
      "\n",
      "Credit card or prepaid card\n",
      "2\n",
      "['Store credit card' 'General-purpose credit card or charge card']\n",
      "1943 Store credit card\n",
      "8057 General-purpose credit card or charge card\n",
      "\n",
      "Debt collection\n",
      "3\n",
      "['Credit card debt' 'Other debt' 'Medical debt']\n",
      "3412 Credit card debt\n",
      "4134 Other debt\n",
      "2454 Medical debt\n",
      "\n",
      "Student loan\n",
      "2\n",
      "['Federal student loan servicing' 'Private student loan']\n",
      "6394 Federal student loan servicing\n",
      "3606 Private student loan\n",
      "\n",
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "1\n",
      "['Credit reporting']\n",
      "10000 Credit reporting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "    print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "    print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print(len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT']], test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63000"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>bait and switch company. They presented me wit...</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Private student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>We accidentally paid XX/XX/XXXX mortgage payme...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>FHA mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62115</th>\n",
       "      <td>I have missed my XXXX mortgage payment. I have...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>FHA mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24155</th>\n",
       "      <td>PHH cashed a check for the wrong amount, it wa...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Conventional home mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36417</th>\n",
       "      <td>I have three hard inquires one is from XXXX XX...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          COMPLAINT_TEXT  \\\n",
       "8125   bait and switch company. They presented me wit...   \n",
       "5941   We accidentally paid XX/XX/XXXX mortgage payme...   \n",
       "62115  I have missed my XXXX mortgage payment. I have...   \n",
       "24155  PHH cashed a check for the wrong amount, it wa...   \n",
       "36417  I have three hard inquires one is from XXXX XX...   \n",
       "\n",
       "                                            MAIN_PRODUCT  \\\n",
       "8125                                        Student loan   \n",
       "5941                                            Mortgage   \n",
       "62115                                           Mortgage   \n",
       "24155                                           Mortgage   \n",
       "36417  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "                      SUB_PRODUCT  \n",
       "8125         Private student loan  \n",
       "5941                 FHA mortgage  \n",
       "62115                FHA mortgage  \n",
       "24155  Conventional home mortgage  \n",
       "36417            Credit reporting  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LENGTH'] = train['COMPLAINT_TEXT'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['COMPLAINT_TEXT'].tolist()\n",
    "train_len = train['LENGTH'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "train_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [doc.lower().replace(r'\\n', '',) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count=bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "def get_corpus(texts):\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 20\n",
    "\n",
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus,\n",
    "                           num_topics=number_of_topics,\n",
    "                           id2word=train_id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.094*\"check\" + 0.044*\"money\" + 0.039*\"send\" + 0.038*\"refund\" + 0.029*\"day\" + 0.028*\"back\" + 0.026*\"company\" + 0.025*\"take\" + 0.024*\"amount\" + 0.020*\"say\" + 0.016*\"bank\" + 0.016*\"return\" + 0.015*\"issue\" + 0.014*\"month\" + 0.014*\"cancel\"'),\n",
       " (1,\n",
       "  '0.017*\"court\" + 0.017*\"law\" + 0.014*\"file\" + 0.014*\"state\" + 0.012*\"act\" + 0.010*\"attorney\" + 0.010*\"legal\" + 0.010*\"right\" + 0.009*\"case\" + 0.009*\"action\" + 0.008*\"consumer\" + 0.008*\"complaint\" + 0.008*\"violation\" + 0.008*\"include\" + 0.007*\"office\"'),\n",
       " (2,\n",
       "  '0.077*\"report\" + 0.051*\"information\" + 0.031*\"remove\" + 0.027*\"dispute\" + 0.024*\"consumer\" + 0.024*\"inquiry\" + 0.021*\"file\" + 0.020*\"verify\" + 0.017*\"equifax\" + 0.015*\"experian\" + 0.014*\"item\" + 0.014*\"request\" + 0.013*\"delete\" + 0.012*\"please\" + 0.012*\"transunion\"'),\n",
       " (3,\n",
       "  '0.112*\"chase\" + 0.072*\"number\" + 0.064*\"address\" + 0.058*\"name\" + 0.025*\"us\" + 0.024*\"information\" + 0.022*\"bank\" + 0.018*\"social_security\" + 0.018*\"phone\" + 0.015*\"mail\" + 0.014*\"open\" + 0.013*\"use\" + 0.013*\"personal\" + 0.013*\"also\" + 0.012*\"send\"'),\n",
       " (4,\n",
       "  '0.106*\"card\" + 0.042*\"charge\" + 0.029*\"fraud\" + 0.025*\"dispute\" + 0.023*\"use\" + 0.023*\"claim\" + 0.019*\"transaction\" + 0.018*\"close\" + 0.015*\"fraudulent\" + 0.012*\"never\" + 0.011*\"merchant\" + 0.011*\"purchase\" + 0.010*\"file\" + 0.008*\"discover\" + 0.008*\"steal\"'),\n",
       " (5,\n",
       "  '0.035*\"speak\" + 0.025*\"phone\" + 0.024*\"ask\" + 0.021*\"representative\" + 0.021*\"contact\" + 0.021*\"could\" + 0.017*\"back\" + 0.017*\"state\" + 0.016*\"say\" + 0.015*\"email\" + 0.015*\"number\" + 0.014*\"day\" + 0.014*\"supervisor\" + 0.013*\"customer_service\" + 0.012*\"advise\"'),\n",
       " (6,\n",
       "  '0.068*\"mortgage\" + 0.055*\"loan\" + 0.028*\"home\" + 0.022*\"modification\" + 0.016*\"escrow\" + 0.015*\"property\" + 0.015*\"insurance\" + 0.012*\"foreclosure\" + 0.010*\"company\" + 0.010*\"lender\" + 0.009*\"house\" + 0.009*\"wells_fargo\" + 0.008*\"ocwen\" + 0.008*\"new\" + 0.007*\"document\"'),\n",
       " (7,\n",
       "  '0.066*\"letter\" + 0.060*\"send\" + 0.047*\"request\" + 0.028*\"document\" + 0.026*\"provide\" + 0.026*\"state\" + 0.021*\"date\" + 0.020*\"copy\" + 0.018*\"email\" + 0.018*\"information\" + 0.017*\"response\" + 0.015*\"mail\" + 0.014*\"submit\" + 0.013*\"complaint\" + 0.013*\"proof\"'),\n",
       " (8,\n",
       "  '0.123*\"report\" + 0.052*\"late\" + 0.024*\"show\" + 0.024*\"score\" + 0.023*\"day\" + 0.019*\"remove\" + 0.017*\"company\" + 0.017*\"never\" + 0.015*\"bureaus\" + 0.015*\"balance\" + 0.014*\"date\" + 0.014*\"correct\" + 0.013*\"information\" + 0.012*\"due\" + 0.012*\"error\"'),\n",
       " (9,\n",
       "  '0.024*\"work\" + 0.019*\"year\" + 0.017*\"income\" + 0.014*\"help\" + 0.013*\"month\" + 0.012*\"could\" + 0.012*\"program\" + 0.012*\"job\" + 0.010*\"option\" + 0.010*\"try\" + 0.009*\"able\" + 0.009*\"due\" + 0.008*\"afford\" + 0.008*\"take\" + 0.008*\"school\"')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.print_topics(number_of_topics, num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             lowercase=True,\n",
    "                             norm='l2',\n",
    "                             max_df=.9,\n",
    "                             min_df=.1)\n",
    "tfidf_text = tfidf_text_vectorizer.fit_transform(train['COMPLAINT_TEXT'])\n",
    "text_cols = tfidf_text_vectorizer.get_feature_names()\n",
    "tfidf_text = pd.DataFrame(tfidf_text.todense(),\n",
    "                          columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(train_text)):\n",
    "    top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(number_of_topics)]\n",
    "    topic_vec.extend(tfidf_text.iloc[i])  # TF-IDF vectors\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap / downsample all classes to have the same number of data points\n",
    "df = rebalance(df, 'MAIN_PRODUCT', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final main-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_vecs)\n",
    "y_train = np.array(train['MAIN_PRODUCT'])\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=100,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['LENGTH'] = test['COMPLAINT_TEXT'].apply(len)\n",
    "test_text = test['COMPLAINT_TEXT'].tolist()\n",
    "test_len = test['LENGTH'].tolist()\n",
    "test_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in test_text]\n",
    "test_text = [doc.lower().replace(r'\\n', '',) for doc in test_text]\n",
    "test_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_test = tfidf_text_vectorizer.transform(test['COMPLAINT_TEXT'])\n",
    "tfidf_text_test = pd.DataFrame(tfidf_text_test.todense(),\n",
    "                               columns=[text_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(texts):\n",
    "    \"\"\"\n",
    "    For the test data we only need the bigram data built on train data,\n",
    "    as we'll use the train id2word mappings. This is a requirement due to \n",
    "    the shapes Gensim functions expect in the test-vector transformation below.\n",
    "    With both these in hand, we can make the test corpus.\n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram\n",
    "  \n",
    "bigram_test = get_bigram(test_text)\n",
    "\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
    "\n",
    "test_vecs = []\n",
    "for i in range(len(test_text)):\n",
    "    top_topics = lda_train.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(number_of_topics)]\n",
    "    topic_vec.extend(tfidf_text_test.iloc[i])\n",
    "    test_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_vecs)\n",
    "y_test = np.array(test['MAIN_PRODUCT'])\n",
    "\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test f1: 0.823 +/- 0.068\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_forest.predict(X_test_scale)\n",
    "rf_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Random Forest Test f1: {np.mean(rf_f1_test):.3f} +/- {np.std(rf_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 0.0009462366793901502\n",
      "Topic 1 0.004309610118597874\n",
      "Topic 2 0.04614970639769628\n",
      "Topic 3 0.0018641663899472297\n",
      "Topic 4 0.030565393463520496\n",
      "Topic 5 0.0013262231637497942\n",
      "Topic 6 0.09839080030478634\n",
      "Topic 7 0.001272344125329751\n",
      "Topic 8 0.010889068443024225\n",
      "Topic 9 0.011832365934081952\n",
      "Topic 10 0.007646975270470209\n",
      "Topic 11 0.005887722545905707\n",
      "Topic 12 0.0616129033738358\n",
      "Topic 13 0.08977795077409001\n",
      "Topic 14 0.0013445573244966836\n",
      "Topic 15 0.06738484568168268\n",
      "Topic 16 0.0013321409349822285\n",
      "Topic 17 0.08444323350243414\n",
      "Topic 18 0.013147649239968093\n",
      "Topic 19 0.0009240718039698565\n"
     ]
    }
   ],
   "source": [
    "feat = random_forest.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])\n",
    "# print('Comment length', feat[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final sub-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test f1: 0.576 +/- 0.153\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_vecs)\n",
    "y_train = np.array(train['SUB_PRODUCT'])\n",
    "\n",
    "main_product = train['MAIN_PRODUCT']\n",
    "labels = LabelEncoder()\n",
    "z = labels.fit_transform(main_product)\n",
    "X_train = np.concatenate((X_train, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "# Scale Data\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scale = scaler_2.fit_transform(X_train)\n",
    "\n",
    "random_forest_2 = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features='auto',\n",
    "    min_samples_leaf=100,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.array(test_vecs)\n",
    "y_test = np.array(test['SUB_PRODUCT'])\n",
    "\n",
    "main_product = random_forest.predict(X_test)\n",
    "z = labels.transform(main_product)\n",
    "X_test = np.concatenate((X_test, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "X_test_scale = scaler_2.transform(X_test)\n",
    "\n",
    "y_pred = random_forest_2.predict(X_test_scale)\n",
    "rf_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Random Forest Test f1: {np.mean(rf_f1_test):.3f} +/- {np.std(rf_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 0.0014033597613339977\n",
      "Topic 1 0.005421063864969904\n",
      "Topic 2 0.026689969814241443\n",
      "Topic 3 0.002411994616475524\n",
      "Topic 4 0.02380467011628789\n",
      "Topic 5 0.0017234509822747716\n",
      "Topic 6 0.07068095209720922\n",
      "Topic 7 0.0018647835775037003\n",
      "Topic 8 0.0075337435385669956\n",
      "Topic 9 0.012436149823865098\n",
      "Topic 10 0.0073302795907105225\n",
      "Topic 11 0.006140538387011782\n",
      "Topic 12 0.039253916005833896\n",
      "Topic 13 0.0625501672187005\n",
      "Topic 14 0.002079433326161963\n",
      "Topic 15 0.05896719801931236\n",
      "Topic 16 0.004856527095735507\n",
      "Topic 17 0.05144747427917818\n",
      "Topic 18 0.011966488230677533\n",
      "Topic 19 0.0017719028533311858\n",
      "Main-product prediction 0.2234067479526026\n"
     ]
    }
   ],
   "source": [
    "feat = random_forest_2.feature_importances_\n",
    "for i in range(number_of_topics):\n",
    "    print('Topic', i, feat[i])\n",
    "# print('Comment length', feat[-2])\n",
    "print('Main-product prediction', feat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit card or prepaid card': 996,\n",
       "             'Debt collection': 1056,\n",
       "             'Student loan': 1001,\n",
       "             'Vehicle loan or lease': 1015,\n",
       "             'Checking or savings account': 996,\n",
       "             'Credit reporting, credit repair services, or other personal consumer reports': 959,\n",
       "             'Mortgage': 977})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = defaultdict(int)\n",
    "for prod in test['MAIN_PRODUCT']:\n",
    "    d1[prod] += 1\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit card or prepaid card': 724,\n",
       "             'Mortgage': 5165,\n",
       "             'Vehicle loan or lease': 853,\n",
       "             'Student loan': 109,\n",
       "             'Debt collection': 39,\n",
       "             'Credit reporting, credit repair services, or other personal consumer reports': 110})"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = defaultdict(int)\n",
    "for pred in random_forest.predict(test_vecs):\n",
    "    d2[pred] += 1\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'General-purpose credit card or charge card': 820,\n",
       "             'Medical debt': 249,\n",
       "             'Private student loan': 365,\n",
       "             'Loan': 1015,\n",
       "             'Checking account': 996,\n",
       "             'Federal student loan servicing': 636,\n",
       "             'Store credit card': 176,\n",
       "             'Credit reporting': 959,\n",
       "             'Other debt': 468,\n",
       "             'Credit card debt': 339,\n",
       "             'Conventional home mortgage': 730,\n",
       "             'FHA mortgage': 247})"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = defaultdict(int)\n",
    "for prod in test['SUB_PRODUCT']:\n",
    "    d3[prod] += 1\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'FHA mortgage': 628,\n",
       "             'Store credit card': 91,\n",
       "             'Loan': 5881,\n",
       "             'Conventional home mortgage': 351,\n",
       "             'Credit card debt': 15,\n",
       "             'General-purpose credit card or charge card': 30,\n",
       "             'Credit reporting': 2,\n",
       "             'Private student loan': 2})"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4 = defaultdict(int)\n",
    "for pred in random_forest_2.predict(X_test):\n",
    "    d4[pred] += 1\n",
    "d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
