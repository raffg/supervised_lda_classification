{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table `compaints_users` for the complaint text and product ID, and the `products` table to find the product and sub-product\n",
    "df = pd.read_csv('data/complaints_users.csv')\n",
    "df2 = pd.read_csv('data/products.csv')\n",
    "df = df.merge(df2, how='left', on='PRODUCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sub-product of \"I do not know\" to null\n",
    "df.loc[df['SUB_PRODUCT'] == 'I do not know', 'SUB_PRODUCT'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs for which there have been no recent complaints\n",
    "df['date_norm'] = df['DATE'].apply(lambda x: int(x[6:10] + x[0:2] + x[3:5]))\n",
    "df = df[df['date_norm'] > 20180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complaints which have null values in either main or sub products\n",
    "df = df[(df['MAIN_PRODUCT'].notnull()) & (df['SUB_PRODUCT'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product IDs which have fewer than 100 complaints\n",
    "df['COMPLAINT_COUNTS'] = df.groupby('SUB_PRODUCT')['COMPLAINT_ID'].transform('count')\n",
    "df = df[df['COMPLAINT_COUNTS'] > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112819"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(df, column, sample_size):\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for group in df[column].unique():\n",
    "        sample = sample_size\n",
    "        data = len(df[df[column] == group])\n",
    "        if data <= sample:\n",
    "            new_df = new_df.append(df[df[column] == group])\n",
    "            sample -= data\n",
    "        new_df = new_df.append(resample(df[df[column] == group], n_samples=sample))\n",
    "    return new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap / downsample all classes to have 1000 data points\n",
    "df = rebalance(df, 'MAIN_PRODUCT', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debt collection\n",
      "3\n",
      "1696 Credit card debt\n",
      "2043 Other debt\n",
      "1261 Medical debt\n",
      "\n",
      "Vehicle loan or lease\n",
      "1\n",
      "5000 Loan\n",
      "\n",
      "Credit card or prepaid card\n",
      "2\n",
      "4030 General-purpose credit card or charge card\n",
      "970 Store credit card\n",
      "\n",
      "Checking or savings account\n",
      "1\n",
      "5000 Checking account\n",
      "\n",
      "Student loan\n",
      "2\n",
      "1746 Private student loan\n",
      "3254 Federal student loan servicing\n",
      "\n",
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "1\n",
      "5000 Credit reporting\n",
      "\n",
      "Mortgage\n",
      "2\n",
      "3763 Conventional home mortgage\n",
      "1237 FHA mortgage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_product in df['MAIN_PRODUCT'].unique():\n",
    "    print(main_product)\n",
    "    print(len(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique()))\n",
    "#     print(df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique())\n",
    "    for sub_product in df[df['MAIN_PRODUCT'] == main_product]['SUB_PRODUCT'].unique():\n",
    "        print(len(df[(df['MAIN_PRODUCT'] == main_product) & (df['SUB_PRODUCT'] == sub_product)]), sub_product)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['COMPLAINT_TEXT', 'MAIN_PRODUCT', 'SUB_PRODUCT']], test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32001</th>\n",
       "      <td>On XX/XX/XXXX my vehicle was repossessed from ...</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>I contacted XXXX XXXX XXXX to have my extended...</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25494</th>\n",
       "      <td>I was calling interested XXXX  phone payment s...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>Store credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>My name is XXXX XXXX. In 2017 my loan funding ...</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Federal student loan servicing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23002</th>\n",
       "      <td>Several months ago we were approved for a loan...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Conventional home mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          COMPLAINT_TEXT  \\\n",
       "32001  On XX/XX/XXXX my vehicle was repossessed from ...   \n",
       "1839   I contacted XXXX XXXX XXXX to have my extended...   \n",
       "25494  I was calling interested XXXX  phone payment s...   \n",
       "34770  My name is XXXX XXXX. In 2017 my loan funding ...   \n",
       "23002  Several months ago we were approved for a loan...   \n",
       "\n",
       "                      MAIN_PRODUCT                     SUB_PRODUCT  \n",
       "32001        Vehicle loan or lease                            Loan  \n",
       "1839         Vehicle loan or lease                            Loan  \n",
       "25494  Credit card or prepaid card               Store credit card  \n",
       "34770                 Student loan  Federal student loan servicing  \n",
       "23002                     Mortgage      Conventional home mortgage  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LENGTH'] = train['COMPLAINT_TEXT'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['COMPLAINT_TEXT'].tolist()\n",
    "train_len = train['LENGTH'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "train_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [doc.lower().replace(r'\\n', '',) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count=bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "def get_corpus(texts):\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 20\n",
    "\n",
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus,\n",
    "                           num_topics=number_of_topics,\n",
    "                           id2word=train_id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"property\" + 0.015*\"law\" + 0.013*\"act\" + 0.012*\"case\" + 0.012*\"note\" + 0.011*\"court\" + 0.009*\"criminal\" + 0.009*\"document\" + 0.008*\"lie\" + 0.008*\"legal\" + 0.008*\"record\" + 0.008*\"never\" + 0.008*\"illegal\" + 0.007*\"state\" + 0.007*\"fee\"'),\n",
       " (1,\n",
       "  '0.103*\"loan\" + 0.029*\"navient\" + 0.020*\"student_loan\" + 0.019*\"year\" + 0.012*\"forbearance\" + 0.011*\"month\" + 0.011*\"program\" + 0.011*\"school\" + 0.011*\"amount\" + 0.009*\"income\" + 0.008*\"monthly\" + 0.008*\"interest\" + 0.008*\"apply\" + 0.008*\"qualify\" + 0.008*\"application\"'),\n",
       " (2,\n",
       "  '0.016*\"financial\" + 0.012*\"damage\" + 0.012*\"agreement\" + 0.012*\"wife\" + 0.010*\"continue\" + 0.009*\"cause\" + 0.008*\"lease\" + 0.007*\"order\" + 0.007*\"attempt\" + 0.006*\"fay\" + 0.006*\"key\" + 0.006*\"action\" + 0.006*\"family\" + 0.006*\"father\" + 0.006*\"accept\"'),\n",
       " (3,\n",
       "  '0.079*\"car\" + 0.043*\"loan\" + 0.040*\"vehicle\" + 0.018*\"finance\" + 0.018*\"purchase\" + 0.012*\"dealership\" + 0.011*\"title\" + 0.010*\"contract\" + 0.010*\"auto\" + 0.009*\"santander\" + 0.009*\"take\" + 0.009*\"dealer\" + 0.009*\"company\" + 0.009*\"sell\" + 0.008*\"go\"'),\n",
       " (4,\n",
       "  '0.070*\"information\" + 0.056*\"inquiry\" + 0.055*\"consumer\" + 0.021*\"agency\" + 0.019*\"block\" + 0.015*\"provide\" + 0.015*\"authorize\" + 0.014*\"without\" + 0.013*\"report\" + 0.011*\"request\" + 0.011*\"consent\" + 0.011*\"apply\" + 0.010*\"file\" + 0.010*\"section\" + 0.010*\"contact\"'),\n",
       " (5,\n",
       "  '0.061*\"mortgage\" + 0.058*\"loan\" + 0.025*\"home\" + 0.020*\"modification\" + 0.011*\"document\" + 0.011*\"property\" + 0.010*\"foreclosure\" + 0.009*\"request\" + 0.009*\"house\" + 0.008*\"letter\" + 0.008*\"lender\" + 0.008*\"date\" + 0.007*\"sell\" + 0.007*\"year\" + 0.007*\"ocwen\"'),\n",
       " (6,\n",
       "  '0.037*\"say\" + 0.029*\"go\" + 0.020*\"back\" + 0.016*\"try\" + 0.016*\"ask\" + 0.015*\"know\" + 0.015*\"could\" + 0.014*\"one\" + 0.013*\"help\" + 0.013*\"want\" + 0.013*\"take\" + 0.012*\"work\" + 0.012*\"money\" + 0.012*\"need\" + 0.011*\"month\"'),\n",
       " (7,\n",
       "  '0.037*\"offer\" + 0.033*\"open\" + 0.033*\"citibank\" + 0.033*\"bonus\" + 0.032*\"citi\" + 0.022*\"day\" + 0.019*\"american_express\" + 0.015*\"promotion\" + 0.015*\"rate\" + 0.013*\"month\" + 0.013*\"purchase\" + 0.013*\"point\" + 0.012*\"contact\" + 0.012*\"term\" + 0.012*\"checking\"'),\n",
       " (8,\n",
       "  '0.021*\"speak\" + 0.021*\"ask\" + 0.021*\"send\" + 0.021*\"phone\" + 0.020*\"email\" + 0.019*\"contact\" + 0.018*\"state\" + 0.017*\"information\" + 0.016*\"say\" + 0.016*\"number\" + 0.016*\"back\" + 0.015*\"representative\" + 0.015*\"day\" + 0.014*\"could\" + 0.014*\"request\"'),\n",
       " (9,\n",
       "  '0.141*\"wells_fargo\" + 0.053*\"bankruptcy\" + 0.047*\"file\" + 0.031*\"attorney\" + 0.031*\"court\" + 0.019*\"case\" + 0.015*\"complaint\" + 0.012*\"chapter\" + 0.011*\"issue\" + 0.010*\"include\" + 0.010*\"discharge\" + 0.009*\"refuse\" + 0.009*\"oh\" + 0.008*\"judgement\" + 0.007*\"wf\"')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.print_topics(number_of_topics, num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(train_text)):\n",
    "    top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(number_of_topics)]\n",
    "    topic_vec.append(train_len[i])  # length of complaint\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search logistic regression hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_vecs)\n",
    "y_train = np.array(train['MAIN_PRODUCT'])\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "\n",
    "cv_gridsearch_f1 = []\n",
    "parameters = {'penalty': ['l1', 'l2'],\n",
    "              'C': [.001, .01, .1, 1, 10, 100],\n",
    "              'class_weight': ['balanced', None]\n",
    "             }\n",
    "clf = GridSearchCV(LogisticRegression(solver='saga', multi_class='auto', max_iter=10000, n_jobs=-1), parameters, cv=10)\n",
    "clf.fit(X_train_scale, y_train)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final main-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_vecs)\n",
    "y_train = np.array(train['MAIN_PRODUCT'])\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "\n",
    "logistic_regression = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=1,\n",
    "    solver='saga',\n",
    "    multi_class='auto',\n",
    "    max_iter=10000,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['LENGTH'] = test['COMPLAINT_TEXT'].apply(len)\n",
    "test_text = test['COMPLAINT_TEXT'].tolist()\n",
    "test_len = test['LENGTH'].tolist()\n",
    "test_text = [' '.join([token.lemma_ for token in nlp(doc)]) for doc in test_text]\n",
    "test_text = [doc.lower().replace(r'\\n', '',) for doc in test_text]\n",
    "test_text = [re.sub(r\"[^a-zA-Z$ -]+\", '', doc) for doc in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(texts):\n",
    "    \"\"\"\n",
    "    For the test data we only need the bigram data built on train data,\n",
    "    as we'll use the train id2word mappings. This is a requirement due to \n",
    "    the shapes Gensim functions expect in the test-vector transformation below.\n",
    "    With both these in hand, we can make the test corpus.\n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(texts))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram\n",
    "  \n",
    "bigram_test = get_bigram(test_text)\n",
    "\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
    "\n",
    "test_vecs = []\n",
    "for i in range(len(test_text)):\n",
    "    top_topics = lda_train.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(number_of_topics)]\n",
    "    topic_vec.append(test_len[i])\n",
    "    test_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_vecs)\n",
    "y_test = np.array(test['MAIN_PRODUCT'])\n",
    "\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test f1: 0.789 +/- 0.078\n"
     ]
    }
   ],
   "source": [
    "y_pred = logistic_regression.predict(X_test_scale)\n",
    "lr_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Logistic Regression Test f1: {np.mean(lr_f1_test):.3f} +/- {np.std(lr_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send test data through final sub-product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_vecs)\n",
    "y_train = np.array(train['SUB_PRODUCT'])\n",
    "\n",
    "main_product = train['MAIN_PRODUCT']\n",
    "labels = LabelEncoder()\n",
    "z = labels.fit_transform(main_product)\n",
    "X_train = np.concatenate((X_train, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "# Scale Data\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_scale = scaler_2.fit_transform(X_train)\n",
    "\n",
    "logistic_regression_2 = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=1,\n",
    "    solver='saga',\n",
    "    multi_class='auto',\n",
    "    max_iter=10000,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ").fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test f1: 0.646 +/- 0.229\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(test_vecs)\n",
    "y_test = np.array(test['SUB_PRODUCT'])\n",
    "\n",
    "main_product = logistic_regression.predict(X_test)\n",
    "main_product = test['MAIN_PRODUCT']\n",
    "z = labels.transform(main_product)\n",
    "X_test = np.concatenate((X_test, z.reshape(len(z), 1)), axis=1)\n",
    "\n",
    "X_test_scale = scaler_2.transform(X_test)\n",
    "\n",
    "y_pred = logistic_regression_2.predict(X_test_scale)\n",
    "lr_f1_test = f1_score(y_test, y_pred, average=None)\n",
    "print(f'Logistic Regression Test f1: {np.mean(lr_f1_test):.3f} +/- {np.std(lr_f1_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Credit card or prepaid card': 481,\n",
       "             'Student loan': 515,\n",
       "             'Credit reporting, credit repair services, or other personal consumer reports': 535,\n",
       "             'Vehicle loan or lease': 504,\n",
       "             'Checking or savings account': 516,\n",
       "             'Mortgage': 487,\n",
       "             'Debt collection': 462})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = defaultdict(int)\n",
    "for prod in test['MAIN_PRODUCT']:\n",
    "    d1[prod] += 1\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Mortgage': 3494, 'Vehicle loan or lease': 6})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = defaultdict(int)\n",
    "for pred in logistic_regression.predict(test_vecs):\n",
    "    d2[pred] += 1\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'General-purpose credit card or charge card': 377,\n",
       "             'Private student loan': 185,\n",
       "             'Credit reporting': 535,\n",
       "             'Loan': 504,\n",
       "             'Checking account': 516,\n",
       "             'Conventional home mortgage': 373,\n",
       "             'Federal student loan servicing': 330,\n",
       "             'Other debt': 191,\n",
       "             'FHA mortgage': 114,\n",
       "             'Medical debt': 138,\n",
       "             'Store credit card': 104,\n",
       "             'Credit card debt': 133})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = defaultdict(int)\n",
    "for prod in test['SUB_PRODUCT']:\n",
    "    d3[prod] += 1\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'FHA mortgage': 1438,\n",
       "             'Loan': 1866,\n",
       "             'Store credit card': 182,\n",
       "             'Private student loan': 5,\n",
       "             'Other debt': 9})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4 = defaultdict(int)\n",
    "for pred in logistic_regression_2.predict(X_test):\n",
    "    d4[pred] += 1\n",
    "d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
